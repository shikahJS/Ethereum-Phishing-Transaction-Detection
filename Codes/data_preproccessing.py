# -*- coding: utf-8 -*-
"""Data Preproccessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OFw9HPVgqliDghOQqIUOlzufM8hElfNF

**Enhanced Phishing Transactions Detection on Ethereum Network with Tree-based Ensembles: An Empirical Study**

* **Researchers:**
  * **Shikah Alsunaidi** (Information and Computer Science Department, KFUPM)
  * **Dr. Hamoud Aljamaan** (Information and Computer Science Department, KFUPM)
---

## ▶ **1. Imports**

---
"""

# Commented out IPython magic to ensure Python compatibility.
# import libraries section
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import gridspec
import seaborn as sns
# %matplotlib inline

# Stop warnings
import warnings as w
w.simplefilter(action='ignore',category=FutureWarning)

"""# ▶ **2 Global Functions**

---

## **2.1. Upload File Function**

---
"""

# This function takes the file name to upload it and return the dataframe
def upload_file(file_name):

  # Upload CSV file (ReducedDS.csv)
  from google.colab import files
  uploaded = files.upload()

  # Convert the uploaded data into dataframe
  import io
  df = pd.read_csv(io.BytesIO(uploaded[file_name]))

  return df

"""## **2.2. Write to File Function**

---

"""

# This function write the dataframe to excel file
def write_to_excel (mcc_DF, model, fileName):
  if model == "RF":
    df=pd.DataFrame(mcc_DF,columns=[model])
    df.to_csv(fileName,index = False)
  else:
    df = pd.read_csv(fileName)
    df[model]=mcc_DF
    df.to_csv(fileName,index = False)

"""# ▶ **3. Load & Explore & Preproccessing the Original Dataset**


---

## ❄ **Al-EMari2021 Dataset**
  * S. Al-E’mari, M. Anbar, Y. Sanjalawe, and S. Manickam, A Labeled Transactions-Based Dataset on the Ethereum Network, vol. 1347, no. February. Springer Singapore, 2021.
  * Multi-lebel
  * https://github.com/salam-ammari/Labeled-Transactions-based-Dataset-of-Ethereum-Network


---

## **3.1 Load DS**

---
"""

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv(io.BytesIO(uploaded['AlEMari2021_DS.csv']))

df.head()

"""## **3.2 Data Exploration & Preproccessing**

---

### **3.2.1. Dataset description**

---
"""

# Dimensions of the dataset
print('The dimension of the dataset is: ', df.shape)

# Dataframe info
df.info()

# Dataframe descriptive statistics
df.describe(include='all')

"""### **3.2.2. Duplication check**


---


"""

# Show all duplicate rows
duplicate = df[df.duplicated(keep = False)]
duplicate

"""#### **A. Drop duplicate rows**


---


"""

if df.duplicated().any():
  df=df.drop_duplicates()
  print('Duplicates dropped')

"""#### **B. New dimension**

---


"""

# Dimensions of the dataset
print('The dimension of the dataset is: ', df.shape)

"""### **3.2.3. Missing data check**

---


"""

df.isnull().sum()

sns.heatmap(df.isnull());

"""### **3.2.4. Degree of class balance**

---


"""

# Data balance (Textual description)
df['from_scam'].value_counts()

# Data balance (Graphical description)
plt.title('Datapoints for each from_scam')
plt.grid('on')
sns.countplot(df['from_scam']);

# Data balance (Textual description)
df['to_scam'].value_counts()

# Data balance (Graphical description)
plt.title('Datapoints for each to_scam')
plt.grid('on')
sns.countplot(df['to_scam']);

# Data balance (Textual description)
df['from_category'].value_counts()

# Data balance (Graphical description)
plt.title('Datapoints for each from_category')
plt.grid('on')
sns.countplot(df['from_category']);

# Data balance (Textual description)
df['to_category'].value_counts()

# Data balance (Graphical description)
plt.title('Datapoints for each to_category')
plt.grid('on')
sns.countplot(df['to_category']);

"""## **3.3 Save the cleaned dataset**

---

"""

df.to_csv('Al-EMari2021_cleaned_DS.csv', index = False)

"""# ▶ **4. Convert the problem into a binary-class problem**


---

### **4.1. Load the cleaned dataset**

---
"""

from google.colab import files
uploaded = files.upload()

import io
df_cleaned = pd.read_csv(io.BytesIO(uploaded['Al-EMari2021_cleaned_DS.csv']))

df_cleaned.head()

"""## **4.2. Copy the cleaned dataset**

---


"""

# Take a copy of the cleaned dataset
bin_data=df_cleaned

# Check the number of rows for each class
bin_data['from_scam'].value_counts()

# Check the number of rows for each class
bin_data['to_scam'].value_counts()

# Check the number of rows for each from_category
bin_data['from_category'].value_counts()

# Check the number of rows for each to_category
bin_data['to_category'].value_counts()

"""## **4.3. Convert into a binary-class problem**
  * Add new feature (a binary class label) called **"class"**, which has two values 0 & 1 as follows.
    * **Class 0:** None Malicious class (Not Attack)
      * **if** to_scam==from_scam == 0
    * **Class 1:** Malicious class (Attack)
      * **if** to_scam **or** from_scam **or** both == 1


---


"""

#  Add Empty Column (class) to Dataframe (bin_data)
bin_data['class'] = np.nan

# Check that the new column (class) has been added to (bin_data)
bin_data.info()

# Change the lable of class 2 to be 0
bin_data['class'] = np.where((bin_data['to_scam'] == 1) | (bin_data['from_scam'] == 1), 1, 0)

"""## **4.4. Remove useless features**
  * from_scam, to_scam, from_category, and to_category features may reveal some information to the ML model.

---


"""

bin_data=bin_data.drop(['from_scam', 'to_scam', 'from_category', 'to_category'], axis=1)

"""## **4.5. Dataset size & classes distribution**

---


"""

# Number of instances in bin_dataset
bin_data.shape

# Check the number of rows for each class
counts = bin_data['class'].value_counts()
counts

#Class ratio in the bin_data
print("Class 0 = %",round(counts[0]*100/counts.sum(),2))
print("Class 1 = %",round(counts[1]*100/counts.sum(),2))

"""## **4.6. Save the cleaned imbalanced binary class dataset**

---

"""

bin_data.to_csv('Final_Cleaned_dataset.csv', index = False)

"""# **▶ 5. Data Preprocessing (Phase #2)**

  ---

## **5.1 Upload the cleaned imbalanced dataset**

---
"""

# Upload CSV file (Final_Cleaned_dataset.csv)
from google.colab import files
uploaded = files.upload()

# Convert the uploaded data into dataframe
import io
df = pd.read_csv(io.BytesIO(uploaded['Final_Cleaned_dataset.csv']))

df

# input feature contains null value (0x) which means empty
# This code convert their hex value into integer

for index, row in df.iterrows():
    if df.loc[index,'input'] == '0x':
      df.loc[index,'input'] = None
    else:
       df.loc[index,'input']=int(df.loc[index,'input'], base=16)

# Convert string block_timestamp into integer
from datetime import datetime

for index, row in df.iterrows():
  if 'UTC' in df.loc[index,'block_timestamp'] :
    df.loc[index,'block_timestamp'] =int(round(datetime.strptime(df.loc[index,'block_timestamp'], '%Y-%m-%d %H:%M:%S %Z').timestamp()))
  else:
    df.loc[index,'block_timestamp']=int(round(datetime.strptime(df.loc[index,'block_timestamp'], '%Y-%m-%d %H:%M:%S%z').timestamp()))

# convert hex columnt into integer
df['from_address']=df['from_address'].apply(int, base=16)
df['to_address']=df['to_address'].apply(int, base=16)
df['block_hash']=df['block_hash'].apply(int, base=16)


'''df['from_address']=float.fromhex(df['from_address'])
df['to_address']=float.fromhex(df['to_address'])
df['block_hash']=float.fromhex(df['block_hash'])'''

# Dataframe descriptive statistics
df.describe(include='all')

# remove unique features from the DS
df=df.drop(['hash'], axis=1)
df.shape

# Dataframe descriptive statistics
df.isnull().sum()

# Remove features with high ratio of missing data from the DS
# Remove input feature
df=df.drop(['input'], axis=1)
df.shape

# Display the final preprocessed dataset
df

# Save the preprocessed dataset
df.to_csv('preprocessed dataset.csv', index = False)