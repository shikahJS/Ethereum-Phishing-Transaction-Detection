{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os, pickle\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_validate, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import matthews_corrcoef, make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, HistGradientBoostingClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import optuna\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings as w\n",
    "w.simplefilter(action='ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OUTER_SPLITS = 10  # outer = evaluation\n",
    "N_REPEATS = 10\n",
    "N_INNER_SPLITS = 5   # inner = tuning\n",
    "N_TRIALS  = 10 \n",
    "N_JOBS = -1\n",
    "pos_label = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Dataset/preprocessed dataset.csv\")\n",
    "\n",
    "X = df.drop(columns=['class'])\n",
    "y = df['class']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Pipeline Builders**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dt_pipeline(k, trial=None, fixed_params=None, scale_pos_weight=None):\n",
    "    if trial is not None:\n",
    "        criterion = trial.suggest_categorical(\"clf__criterion\", [\"gini\", \"entropy\"])\n",
    "        max_depth = trial.suggest_int(\"clf__max_depth\", 2, 8)\n",
    "    else:\n",
    "        criterion = fixed_params[\"clf__criterion\"]\n",
    "        max_depth = fixed_params[\"clf__max_depth\"]\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, class_weight=\"balanced\")\n",
    "\n",
    "    return Pipeline([('scaler', StandardScaler()),\n",
    "                     ('select', SelectKBest(mutual_info_classif, k=k)),\n",
    "                     ('clf', clf)])\n",
    "#--------------------------------------\n",
    "def build_rf_pipeline(k, trial=None, fixed_params=None, scale_pos_weight=None):\n",
    "    if trial is not None:\n",
    "        n_estimators = trial.suggest_int(\"clf__n_estimators\", 50, 100)\n",
    "        max_depth = trial.suggest_int(\"clf__max_depth\", 2, 8)\n",
    "    else:\n",
    "        n_estimators = fixed_params[\"clf__n_estimators\"]\n",
    "        max_depth = fixed_params[\"clf__max_depth\"]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, class_weight=\"balanced\", n_jobs=N_JOBS) \n",
    "\n",
    "    return Pipeline([('scaler', StandardScaler()),\n",
    "                     ('select', SelectKBest(mutual_info_classif, k=k)),\n",
    "                     ('clf', clf)])\n",
    "#--------------------------------------\n",
    "def build_et_pipeline(k, trial=None, fixed_params=None, scale_pos_weight=None):\n",
    "    if trial is not None:\n",
    "        n_estimators = trial.suggest_int(\"clf__n_estimators\", 50, 100)\n",
    "        max_depth = trial.suggest_int(\"clf__max_depth\", 2, 8)\n",
    "    else:\n",
    "        n_estimators = fixed_params[\"clf__n_estimators\"]\n",
    "        max_depth = fixed_params[\"clf__max_depth\"]\n",
    "\n",
    "    clf = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth, class_weight=\"balanced\", n_jobs=N_JOBS) \n",
    "\n",
    "    return Pipeline([('scaler', StandardScaler()),\n",
    "                     ('select', SelectKBest(mutual_info_classif, k=k)),\n",
    "                     ('clf', clf)])\n",
    "#--------------------------------------\n",
    "def build_xgb_pipeline(k, trial=None, fixed_params=None, scale_pos_weight=None):\n",
    "    if trial is not None:\n",
    "        n_estimators = trial.suggest_categorical(\"clf__n_estimators\", [100, 300, 500, 700, 900])\n",
    "        learning_rate = trial.suggest_float(\"clf__learning_rate\", 0.1, 0.7)\n",
    "        max_depth = trial.suggest_int(\"clf__max_depth\", 2, 8)\n",
    "    else:\n",
    "        n_estimators = fixed_params[\"clf__n_estimators\"]\n",
    "        learning_rate = fixed_params[\"clf__learning_rate\"]\n",
    "        max_depth = fixed_params[\"clf__max_depth\"]\n",
    "\n",
    "    clf = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, n_jobs=N_JOBS,\n",
    "                        scale_pos_weight=scale_pos_weight) \n",
    "\n",
    "    return Pipeline([('scaler', StandardScaler()),\n",
    "                     ('select', SelectKBest(mutual_info_classif, k=k)),\n",
    "                     ('clf', clf)])\n",
    "#--------------------------------------\n",
    "def build_ada_pipeline(k, trial=None, fixed_params=None, scale_pos_weight=None):\n",
    "    if trial is not None:\n",
    "        n_estimators = trial.suggest_categorical(\"clf__n_estimators\", [100, 300, 500, 700, 900])\n",
    "        learning_rate = trial.suggest_float(\"clf__learning_rate\", 0.01, 1.0)\n",
    "    else:\n",
    "        n_estimators = fixed_params[\"clf__n_estimators\"]\n",
    "        learning_rate = fixed_params[\"clf__learning_rate\"]\n",
    "\n",
    "    base_tree = DecisionTreeClassifier(max_depth=1, class_weight=\"balanced\")\n",
    "\n",
    "    clf = AdaBoostClassifier(estimator=base_tree, n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "\n",
    "    return Pipeline([('scaler', StandardScaler()),\n",
    "                     ('select', SelectKBest(mutual_info_classif, k=k)),\n",
    "                     ('clf', clf)])\n",
    "#--------------------------------------\n",
    "def build_gb_pipeline(k, trial=None, fixed_params=None, scale_pos_weight=None):\n",
    "    if trial is not None:\n",
    "        n_estimators = trial.suggest_categorical(\"clf__n_estimators\", [50, 100, 500, 1000])\n",
    "        learning_rate = trial.suggest_categorical(\"clf__learning_rate\", [0.001, 0.01, 0.1])\n",
    "        min_samples_leaf = trial.suggest_categorical(\"clf__min_samples_leaf\", [1, 5, 10])\n",
    "        max_depth = trial.suggest_int(\"clf__max_depth\", 2, 8)\n",
    "        loss = trial.suggest_categorical(\"clf__loss\", [\"deviance\", \"exponential\"])\n",
    "    else:\n",
    "        n_estimators = fixed_params[\"clf__n_estimators\"]\n",
    "        learning_rate = fixed_params[\"clf__learning_rate\"]\n",
    "        min_samples_leaf = fixed_params[\"clf__min_samples_leaf\"]\n",
    "        max_depth = fixed_params[\"clf__max_depth\"]\n",
    "        loss = fixed_params[\"clf__loss\"]\n",
    "\n",
    "    # sklearn rename: \"deviance\" -> \"log_loss\"\n",
    "    sklearn_loss = \"log_loss\" if loss == \"deviance\" else loss\n",
    "\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, min_samples_leaf=min_samples_leaf, \n",
    "                                     max_depth=max_depth, loss=sklearn_loss)\n",
    "\n",
    "    return Pipeline([('scaler', StandardScaler()),\n",
    "                     ('select', SelectKBest(mutual_info_classif, k=k)),\n",
    "                     ('clf', clf)])\n",
    "#--------------------------------------\n",
    "def build_hgb_pipeline(k, trial=None, fixed_params=None, scale_pos_weight=None):\n",
    "    if trial is not None:\n",
    "        max_iter = trial.suggest_categorical(\"clf__max_iter\", [100, 300, 500, 700, 900])\n",
    "        learning_rate = trial.suggest_float(\"clf__learning_rate\", 0.1, 0.7)\n",
    "        min_samples_leaf = trial.suggest_int(\"clf__min_samples_leaf\", 5, 25)\n",
    "        max_depth = trial.suggest_int(\"clf__max_depth\", 2, 8)\n",
    "    else:\n",
    "        max_iter = fixed_params[\"clf__max_iter\"]\n",
    "        learning_rate = fixed_params[\"clf__learning_rate\"]\n",
    "        min_samples_leaf = fixed_params[\"clf__min_samples_leaf\"]\n",
    "        max_depth = fixed_params[\"clf__max_depth\"]\n",
    "\n",
    "    clf = HistGradientBoostingClassifier(max_iter=max_iter, learning_rate=learning_rate, min_samples_leaf=min_samples_leaf, max_depth=max_depth)\n",
    "\n",
    "    return Pipeline([('scaler', StandardScaler()),\n",
    "                     ('select', SelectKBest(mutual_info_classif, k=k)),\n",
    "                     ('clf', clf)])\n",
    "#--------------------------------------\n",
    "def build_cat_pipeline(k, trial=None, fixed_params=None, scale_pos_weight=None):\n",
    "    if trial is not None:\n",
    "        n_estimators = trial.suggest_categorical(\"clf__n_estimators\", [100, 300, 500, 700, 900]) #CatBoost uses iterations\n",
    "        learning_rate = trial.suggest_float(\"clf__learning_rate\", 0.1, 0.7)\n",
    "        depth = trial.suggest_int(\"clf__depth\", 2, 8)\n",
    "        min_data_in_leaf = trial.suggest_int(\"clf__min_data_in_leaf\", 1, 10)\n",
    "    else:\n",
    "        n_estimators = fixed_params[\"clf__n_estimators\"]\n",
    "        learning_rate = fixed_params[\"clf__learning_rate\"]\n",
    "        depth = fixed_params[\"clf__depth\"]\n",
    "        min_data_in_leaf = fixed_params[\"clf__min_data_in_leaf\"]\n",
    "    \n",
    "    clf = CatBoostClassifier(iterations=n_estimators, learning_rate=learning_rate, depth=depth, min_data_in_leaf=min_data_in_leaf,\n",
    "                             auto_class_weights=\"Balanced\", verbose=False)\n",
    "    \n",
    "    return Pipeline([('scaler', StandardScaler()),\n",
    "                     ('select', SelectKBest(mutual_info_classif, k=k)),\n",
    "                     ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_k_from_mi(X_train, y_train, rule=\"mean\"):\n",
    "    mi = mutual_info_classif(X_train, y_train)\n",
    "    thr = mi.mean() if rule == \"mean\" else np.median(mi)\n",
    "    k = int((mi >= thr).sum())\n",
    "    return k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Nested CV**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  - Outer: RepeatedStratifiedKFold (10 folds x 10 repeats)\n",
    "#  - Inner: StratifiedKFold for Optuna objective via cross_val_score\n",
    "# -------------------------\n",
    "def nested_cv_model(X, y, model_name, build_pipeline_fn, n_outer_splits=N_OUTER_SPLITS, n_repeats=N_REPEATS, n_inner_splits=N_INNER_SPLITS, \n",
    "              n_trials=N_TRIALS):\n",
    "    \n",
    "    outer_cv = RepeatedStratifiedKFold(n_splits=n_outer_splits, n_repeats=n_repeats)\n",
    "\n",
    "    outer_mcc = []\n",
    "    outer_acc = []\n",
    "    outer_precision = []\n",
    "    outer_recall = []\n",
    "    outer_f1 = []\n",
    "    outer_auc = []\n",
    "    outer_inf_time = []\n",
    "\n",
    "    fold_idx = 0\n",
    "\n",
    "    for train_idx, test_idx in outer_cv.split(X, y):\n",
    "        fold_idx += 1\n",
    "        print(f\"[{model_name}] Outer split {fold_idx}/{outer_cv.get_n_splits()}\")\n",
    "\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        #-----------------------------------------------------\n",
    "        pos = (y_train == pos_label).sum()\n",
    "        neg = (y_train != pos_label).sum()\n",
    "        fold_scale_pos_weight = neg / pos\n",
    "        #-----------------------------------------------------\n",
    "        k = choose_k_from_mi(X_train, y_train, rule=\"mean\")\n",
    "        #-----------------------------------------------------\n",
    "        def objective(trial):\n",
    "            pipe = build_pipeline_fn(k, trial=trial, scale_pos_weight=fold_scale_pos_weight)\n",
    "            inner_cv = StratifiedKFold(n_splits=n_inner_splits, shuffle=True)\n",
    "            cv_scores = cross_val_score(pipe, X_train, y_train, cv=inner_cv, scoring=make_scorer(matthews_corrcoef), n_jobs=N_JOBS)\n",
    "            return float(cv_scores.mean())\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best params: {best_params}\")\n",
    "        #-----------------------------------------------------\n",
    "        # Refit best pipeline on full outer training data\n",
    "        best_pipe = build_pipeline_fn(k, fixed_params=best_params, scale_pos_weight=fold_scale_pos_weight)\n",
    "        best_pipe.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on outer test fold\n",
    "        start_time = time.perf_counter()\n",
    "        y_pred = best_pipe.predict(X_test)\n",
    "        end_time = time.perf_counter()\n",
    "        #-----------------------------------------------------\n",
    "        avg_inf_time = (end_time - start_time) / len(X_test)\n",
    "        outer_inf_time.append(avg_inf_time)\n",
    "\n",
    "        # Probabilities for AUC if available\n",
    "        if hasattr(best_pipe, \"predict_proba\"):\n",
    "            y_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            try:\n",
    "                scores_dec = best_pipe.decision_function(X_test)\n",
    "                scores_dec = (scores_dec - scores_dec.min()) / (scores_dec.max() - scores_dec.min() + 1e-9)\n",
    "                y_proba = scores_dec\n",
    "            except Exception:\n",
    "                y_proba = None\n",
    "        #-----------------------------------------------------\n",
    "        # Metrics\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        auc = roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "\n",
    "        outer_mcc.append(mcc)\n",
    "        outer_acc.append(acc)\n",
    "        outer_precision.append(precision)\n",
    "        outer_recall.append(recall)\n",
    "        outer_f1.append(f1)\n",
    "        outer_auc.append(auc)\n",
    "\n",
    "        print(f\"Split {fold_idx}: MCC={mcc:.4f}, ACC={acc:.4f}, \"\n",
    "              f\"Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}, AUC={auc:.4f}, \"\n",
    "              f\"AvgInfTime={avg_inf_time:.6f}s\")\n",
    "\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"mcc_scores\": outer_mcc,\n",
    "        \"acc_scores\": outer_acc,\n",
    "        \"precision_scores\": outer_precision,\n",
    "        \"recall_scores\": outer_recall,\n",
    "        \"f1_scores\": outer_f1,\n",
    "        \"auc_scores\": outer_auc,\n",
    "        \"inf_time_scores\": outer_inf_time,\n",
    "\n",
    "        \"mcc_mean\": float(np.mean(outer_mcc)),\n",
    "        \"mcc_std\": float(np.std(outer_mcc)),\n",
    "        \"acc_mean\": float(np.mean(outer_acc)),\n",
    "        \"precision_mean\": float(np.mean(outer_precision)),\n",
    "        \"recall_mean\": float(np.mean(outer_recall)),\n",
    "        \"f1_mean\": float(np.mean(outer_f1)),\n",
    "        \"auc_mean\": float(np.nanmean(outer_auc)),\n",
    "        \"inf_time_mean\": float(np.mean(outer_inf_time))\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Main**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_config = {\"DT\":  build_dt_pipeline,\n",
    "                 \"RF\":  build_rf_pipeline,\n",
    "                 \"ET\":  build_et_pipeline,\n",
    "                 \"XGB\": build_xgb_pipeline,\n",
    "                 \"Ada\": build_ada_pipeline,\n",
    "                 \"GB\":  build_gb_pipeline,\n",
    "                 \"HGB\": build_hgb_pipeline,\n",
    "                 \"CAT\": build_cat_pipeline}\n",
    "\n",
    "os.makedirs(\"./Results\", exist_ok=True)\n",
    "os.makedirs(\"./Results/all\", exist_ok=True)\n",
    "os.makedirs(\"./Results/per_model\", exist_ok=True)\n",
    "\n",
    "results_all = []\n",
    "summary_rows = []\n",
    "metric_scores = {'MCC': {}, 'ACC': {}, 'Precision': {},'Recall': {}, 'F1': {}, 'AUC': {}, 'INF_TIME': {}}\n",
    "\n",
    "for model_name, builder_fn in models_config.items():\n",
    "\n",
    "    pkl_path = f\"./Results/per_model/{model_name}_nested_cv_result.pkl\"\n",
    "    csv_scores_path = f\"./Results/per_model/{model_name}_fold_scores.csv\"\n",
    "\n",
    "    #Resume behavior, load if already computed\n",
    "    if os.path.exists(pkl_path):\n",
    "        print(f\"[{model_name}] Found saved result -> loading: {pkl_path}\")\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            res = pickle.load(f)\n",
    "    else:\n",
    "        print(f\"[{model_name}] Running nested CV ...\")\n",
    "        res = nested_cv_model(X=X, y=y, model_name=model_name, build_pipeline_fn=builder_fn, n_outer_splits=N_OUTER_SPLITS, n_repeats=N_REPEATS,\n",
    "                          n_inner_splits=N_INNER_SPLITS, n_trials=N_TRIALS)\n",
    "        \n",
    "        with open(pkl_path, \"wb\") as f:\n",
    "            pickle.dump(res, f)\n",
    "\n",
    "    #Per-model fold scores\n",
    "    per_model_scores = pd.DataFrame({\"MCC\": res[\"mcc_scores\"],\n",
    "                                     \"ACC\": res[\"acc_scores\"],\n",
    "                                     \"Precision\": res[\"precision_scores\"],\n",
    "                                     \"Recall\": res[\"recall_scores\"],\n",
    "                                     \"F1\": res[\"f1_scores\"],\n",
    "                                     \"AUC\": res[\"auc_scores\"],\n",
    "                                     \"INF_TIME\": res[\"inf_time_scores\"]\n",
    "                                    })\n",
    "    per_model_scores.to_csv(csv_scores_path, index=False)\n",
    "\n",
    "    #Update in-memory aggregations\n",
    "    results_all.append(res)\n",
    "\n",
    "    summary_rows.append({\"Model\": model_name,\n",
    "                         \"MCC_mean\": res[\"mcc_mean\"],\n",
    "                         \"MCC_std\":  res[\"mcc_std\"],\n",
    "                         \"ACC_mean\": res[\"acc_mean\"],\n",
    "                         \"PREC_mean\": res[\"precision_mean\"],\n",
    "                         \"REC_mean\":  res[\"recall_mean\"],\n",
    "                         \"F1_mean\":   res[\"f1_mean\"],\n",
    "                         \"AUC_mean\":  res[\"auc_mean\"],\n",
    "                         \"INF_TIME_mean\": res[\"inf_time_mean\"]\n",
    "                        })\n",
    "\n",
    "    metric_scores[\"MCC\"][model_name] = res[\"mcc_scores\"]\n",
    "    metric_scores[\"ACC\"][model_name] = res[\"acc_scores\"]\n",
    "    metric_scores[\"Precision\"][model_name] = res[\"precision_scores\"]\n",
    "    metric_scores[\"Recall\"][model_name] = res[\"recall_scores\"]\n",
    "    metric_scores[\"F1\"][model_name] = res[\"f1_scores\"]\n",
    "    metric_scores[\"AUC\"][model_name] = res[\"auc_scores\"]\n",
    "    metric_scores[\"INF_TIME\"][model_name] = res[\"inf_time_scores\"]\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    summary_df.to_csv(\"./Results/summary_performance_metrics.csv\", index=False)\n",
    "    print(summary_df)\n",
    "\n",
    "    #All metrics\" files\n",
    "    pd.DataFrame(metric_scores[\"ACC\"]).to_csv(\"./Results/all/ACC_Scores.csv\", index=False)\n",
    "    pd.DataFrame(metric_scores[\"MCC\"]).to_csv(\"./Results/all/MCC_Scores.csv\", index=False)\n",
    "    pd.DataFrame(metric_scores[\"Precision\"]).to_csv(\"./Results/all/Precision_Scores.csv\", index=False)\n",
    "    pd.DataFrame(metric_scores[\"Recall\"]).to_csv(\"./Results/all/Recall_Scores.csv\", index=False)\n",
    "    pd.DataFrame(metric_scores[\"F1\"]).to_csv(\"./Results/all/F1_Scores.csv\", index=False)\n",
    "    pd.DataFrame(metric_scores[\"AUC\"]).to_csv(\"./Results/all/AUC_Scores.csv\", index=False)\n",
    "    pd.DataFrame(metric_scores[\"INF_TIME\"]).to_csv(\"./Results/all/INF_TIME_Scores.csv\", index=False)\n",
    "\n",
    "print(\"Done. Per-model results saved in ./Results/per_model/ and combined scores in ./Results/all/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = \"./Results/summary_performance_metrics.csv\"\n",
    "\n",
    "# Order must match your paper\n",
    "MODEL_ORDER = [\"DT\", \"RF\", \"ET\", \"XGB\", \"Ada\", \"GB\", \"HGB\", \"CAT\"]\n",
    "\n",
    "# Columns in your summary CSV (based on your code)\n",
    "COL_MAP = {\n",
    "    \"Accuracy\": \"ACC_mean\",\n",
    "    \"Precision\": \"PREC_mean\",\n",
    "    \"Recall\": \"REC_mean\",\n",
    "    \"AUC\": \"AUC_mean\",\n",
    "    \"MCC\": \"MCC_mean\",\n",
    "    # time in seconds in your code -> convert to microseconds\n",
    "    \"Detection Time (in microseconds)\": \"INF_TIME_mean\",\n",
    "}\n",
    "\n",
    "# Which direction is better?\n",
    "HIGHER_BETTER = {\"Accuracy\", \"Precision\", \"Recall\", \"AUC\", \"MCC\"}\n",
    "LOWER_BETTER = {\"Detection Time (in microseconds)\"}\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Ensure we have expected models and ordering\n",
    "df[\"Model\"] = df[\"Model\"].astype(str)\n",
    "df = df.set_index(\"Model\").reindex(MODEL_ORDER)\n",
    "\n",
    "missing = df.index[df.isna().all(axis=1)].tolist()\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing models in CSV (or all-NaN rows): {missing}\")\n",
    "\n",
    "# Convert time to microseconds\n",
    "df[\"INF_TIME_us\"] = df[\"INF_TIME_mean\"] * 1_000_000.0\n",
    "\n",
    "def fmt4(x: float) -> str:\n",
    "    return f\"{x:.4f}\"\n",
    "\n",
    "# Build rows (with bold best)\n",
    "rows = []\n",
    "for metric_name, col in COL_MAP.items():\n",
    "    if metric_name == \"Detection Time (in microseconds)\":\n",
    "        values = df[\"INF_TIME_us\"]\n",
    "        best_model = values.idxmin()\n",
    "    else:\n",
    "        values = df[col]\n",
    "        best_model = values.idxmax()\n",
    "\n",
    "    formatted = []\n",
    "    for m in MODEL_ORDER:\n",
    "        v = values.loc[m]\n",
    "        cell = fmt4(v)\n",
    "        if m == best_model:\n",
    "            cell = r\"\\textbf{\" + cell + \"}\"\n",
    "        formatted.append(cell)\n",
    "\n",
    "    rows.append((metric_name, formatted))\n",
    "\n",
    "# Create LaTeX\n",
    "latex = []\n",
    "latex.append(r\"\\begin{table}[hbt!]\")\n",
    "latex.append(r\"\\caption{Tree-based ensembles performance scores.}\")\n",
    "latex.append(r\"\\label{tab:tree_based_ensemble_performance}\")\n",
    "latex.append(r\"\\scriptsize\")\n",
    "latex.append(r\"\\centering\")\n",
    "latex.append(r\"\\begin{tabular}{lcccccccc}\")\n",
    "latex.append(r\"\\toprule\")\n",
    "latex.append(r\"\\textbf{Classifier} & \\textbf{DT} & \\textbf{RF} & \\textbf{ET} & \\textbf{XGB} & \\textbf{Ada} & \\textbf{GB} & \\textbf{HGB} & \\textbf{CAT} \\\\\")\n",
    "latex.append(r\"\\midrule\")\n",
    "\n",
    "for metric_name, vals in rows:\n",
    "    latex.append(metric_name + \" & \" + \" & \".join(vals) + r\" \\\\\")\n",
    "\n",
    "latex.append(r\"\\bottomrule\")\n",
    "latex.append(r\"\\end{tabular}\")\n",
    "latex.append(r\"\\end{table}\")\n",
    "\n",
    "print(\"\\n\".join(latex))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Statistical Test**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCC_Scores = pd.read_csv(\"./Results/all/MCC_Scores.csv\")\n",
    "models = ['DT', 'RF', 'ET', 'XGB', 'Ada', 'GB', 'HGB', 'CAT']\n",
    "\n",
    "for m in models:\n",
    "    MCC_Scores[m] = pd.to_numeric(MCC_Scores[m], errors=\"coerce\")\n",
    "\n",
    "MCC_Scores = MCC_Scores.dropna(subset=models, how=\"any\")\n",
    "\n",
    "alpha = 0.05\n",
    "peers = len(models) - 1\n",
    "\n",
    "decision_matrix = pd.DataFrame(0, index=models, columns=models, dtype=\"Int64\")\n",
    "pvals = pd.DataFrame(np.nan, index=models, columns=models)\n",
    "\n",
    "for i in models:\n",
    "    xi = MCC_Scores[i].to_numpy()\n",
    "    mean_i = float(np.mean(xi))\n",
    "\n",
    "    for j in models:\n",
    "        if i == j:\n",
    "            decision_matrix.loc[i, j] = pd.NA\n",
    "            continue\n",
    "\n",
    "        xj = MCC_Scores[j].to_numpy()\n",
    "        mean_j = float(np.mean(xj))\n",
    "\n",
    "        diff = xi - xj\n",
    "        if np.all(diff == 0):\n",
    "            stat = 0.0\n",
    "            p = 1.0\n",
    "        else:\n",
    "            stat, p = wilcoxon(xi, xj) #zero_method=\"zsplit\"\n",
    "\n",
    "        pvals.loc[i, j] = p\n",
    "\n",
    "        print(f\"Statistics={stat:.8f}, p={p:.8f}\")\n",
    "        print(f\"The test between {i} and {j}\")\n",
    "        if p > alpha:\n",
    "            print(\"Same prediction performance (Fail to reject H0)\")\n",
    "        else:\n",
    "            print(\"Different prediction performance (Reject H0)\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # 1 only if i significantly better than j (mean-based direction)\n",
    "        decision_matrix.loc[i, j] = 1 if (p <= alpha and mean_i > mean_j) else 0\n",
    "\n",
    "# wins/losses\n",
    "#-----------------------\n",
    "wins = decision_matrix[models].sum(axis=1, skipna=True).astype(int)\n",
    "losses = decision_matrix[models].sum(axis=0, skipna=True).astype(int)\n",
    "\n",
    "win_pct = (wins / peers * 100).round().astype(int).astype(str) + \"%\"\n",
    "loss_pct = (losses / peers * 100).round().astype(int).astype(str) + \"%\"\n",
    "\n",
    "decision_matrix_object = decision_matrix.astype(\"object\")\n",
    "\n",
    "for m in models:\n",
    "    decision_matrix_object.loc[m, m] = \"-\"\n",
    "\n",
    "for i in models:\n",
    "    for j in models:\n",
    "        if i != j and decision_matrix_object.loc[i, j] == 0:\n",
    "            decision_matrix_object.loc[i, j] = \"\"\n",
    "\n",
    "decision_matrix_object[\"Win\"] = wins.astype(object)\n",
    "decision_matrix_object[\"Win %\"] = win_pct.astype(object)\n",
    "\n",
    "decision_matrix_object.loc[\"Loss\", models] = losses.values\n",
    "decision_matrix_object.loc[\"Loss\", \"Win\"] = \"\"\n",
    "decision_matrix_object.loc[\"Loss\", \"Win %\"] = \"\"\n",
    "\n",
    "decision_matrix_object.loc[\"Loss %\", models] = loss_pct.values\n",
    "decision_matrix_object.loc[\"Loss %\", \"Win\"] = \"\"\n",
    "decision_matrix_object.loc[\"Loss %\", \"Win %\"] = \"\"\n",
    "\n",
    "os.makedirs(\"./Results/StatTest\", exist_ok=True)\n",
    "decision_matrix_object.to_csv(\"./Results/StatTest/Wilcoxon_Test_Result.csv\", index=True)\n",
    "pvals.to_csv(\"./Results/StatTest/Wilcoxon_PValues.csv\", index=True)\n",
    "\n",
    "decision_matrix_object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Plots**\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MI feature ranking**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10)\n",
    "n_folds = cv.get_n_splits()\n",
    "feature_names = np.array(X.columns)\n",
    "selected_counts = pd.Series(0, index=feature_names, dtype=int)\n",
    "mi_sum = pd.Series(0.0, index=feature_names, dtype=float)\n",
    "mi_sq_sum = pd.Series(0.0, index=feature_names, dtype=float)\n",
    "\n",
    "# -------------------------\n",
    "# Compute MI per fold on TRAIN split\n",
    "# -------------------------\n",
    "for train_idx, _ in cv.split(X, y):\n",
    "    X_train = X.iloc[train_idx]\n",
    "    y_train = y.iloc[train_idx]\n",
    "\n",
    "    mi = mutual_info_classif(X_train, y_train)\n",
    "    mi_s = pd.Series(mi, index=feature_names)\n",
    "\n",
    "    k = choose_k_from_mi(X_train, y_train, rule=\"mean\")\n",
    "    selected = mi_s.sort_values(ascending=False).iloc[:k].index\n",
    "\n",
    "    selected_counts.loc[selected] += 1\n",
    "    mi_sum += mi_s\n",
    "    mi_sq_sum += (mi_s ** 2)\n",
    "\n",
    "# -------------------------\n",
    "# Aggregate fold-wise stats\n",
    "# -------------------------\n",
    "mi_mean = mi_sum / n_folds\n",
    "mi_std = np.sqrt((mi_sq_sum / n_folds) - (mi_mean ** 2))\n",
    "\n",
    "feature_cv_summary = pd.DataFrame({\"selected_freq\": selected_counts, \"selected_pct\": (selected_counts / n_folds) * 100.0, \"mi_mean\": mi_mean, \n",
    "                                   \"mi_std\": mi_std}).sort_values([\"selected_freq\", \"mi_mean\"], ascending=[False, False])\n",
    "\n",
    "feature_cv_summary.to_csv(os.path.join(\"./Results/CV_Selected_Features_Summary.csv\"), index=True)\n",
    "\n",
    "# -------------------------\n",
    "# Plot 1: Selection frequency (%)\n",
    "# -------------------------\n",
    "top_n = 30\n",
    "top_freq = feature_cv_summary.head(top_n)\n",
    "\n",
    "fig1 = plt.figure(figsize=(12, 6))\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Selection Frequency (%)\")\n",
    "plt.title(\"Most Important Transaction Features\")\n",
    "\n",
    "bars1 = plt.bar(top_freq.index, top_freq[\"selected_pct\"].values, width=0.5, color=(0.2, 0.4, 0.6, 0.6))\n",
    "for bar, val in zip(bars1, top_freq[\"selected_pct\"].values):\n",
    "    y_pos = bar.get_height() / 2 if bar.get_height() >= 0.05 else bar.get_height() + 0.02\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, y_pos, f\"{val:.0f}\", ha=\"center\", va=\"center\", rotation=90,\n",
    "             fontsize=9, color=\"black\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(True, color=\"grey\", linewidth=0.3, linestyle=\"-.\")\n",
    "plt.tight_layout()\n",
    "\n",
    "fig1.savefig(os.path.join(\"./Results/Figures/CV_Feature_Selection_Frequency.pdf\"), format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# Plot 2: CV-averaged MI ranking\n",
    "# -------------------------\n",
    "top_mi = feature_cv_summary.sort_values(\"mi_mean\", ascending=False).head(top_n)\n",
    "\n",
    "fig2 = plt.figure(figsize=(12, 6))\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Averaged MI\")\n",
    "plt.title(\"Features Ranking Based on MI Value\")\n",
    "\n",
    "bars2 = plt.bar(top_mi.index, top_mi[\"mi_mean\"].values, width=0.5, color=(0.2, 0.4, 0.6, 0.6))\n",
    "\n",
    "for bar, val in zip(bars2, top_mi[\"mi_mean\"].values):\n",
    "    y_pos = bar.get_height() / 2 if bar.get_height() >= 0.05 else bar.get_height() + 0.1\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2,y_pos, f\"{val:.4f}\", ha=\"center\", va=\"center\", rotation=90, \n",
    "             fontsize=9, color=\"black\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(True, color=\"grey\", linewidth=0.3, linestyle=\"-.\")\n",
    "plt.tight_layout()\n",
    "\n",
    "fig2.savefig(os.path.join(\"./Results/Figures/CV_MI_Ranking.pdf\"), format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "feature_cv_summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Precision & Recall bar chart (per model)**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREC_Scores = pd.read_csv(\"./Results/all/Precision_Scores.csv\")\n",
    "REC_Scores  = pd.read_csv(\"./Results/all/Recall_Scores.csv\")\n",
    "\n",
    "models = ['DT', 'RF', 'ET', 'XGB', 'Ada', 'GB', 'HGB', 'CAT']\n",
    "\n",
    "for df in [PREC_Scores, REC_Scores]:\n",
    "    for c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "mean_Precision_Scores = [float(PREC_Scores[m].mean()) for m in models]\n",
    "mean_Recall_Scores    = [float(REC_Scores[m].mean()) for m in models]\n",
    "Precision_And_Recall_scores = mean_Precision_Scores + mean_Recall_Scores\n",
    "Precision_And_Recall_scores_DF = pd.DataFrame({'Classifier': models * 2, 'Score': Precision_And_Recall_scores,\n",
    "                                               'Evaluation Metrics': (['Precision'] * len(models)) + (['Recall'] * len(models))})\n",
    "\n",
    "sns.set(style='ticks')\n",
    "g = sns.catplot(x='Classifier', y='Score', hue='Evaluation Metrics', data=Precision_And_Recall_scores_DF, kind='bar', height=4, \n",
    "                aspect=2.5, palette=\"PuBu\")\n",
    "\n",
    "ax = g.facet_axis(0, 0)\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    if (h is None) or np.isnan(h) or h <= 0:\n",
    "        continue\n",
    "    ax.text(p.get_x() + p.get_width() / 2, h / 2, f\"{h:.4f}\", color='black', ha=\"center\", va=\"center\", rotation='vertical', size='small')\n",
    "\n",
    "plt.title('Precision and Recall for Each Tree-based Classifier', fontsize=12)\n",
    "plt.grid(True, color=\"grey\", which='major', linewidth=\"0.3\", linestyle=\"-.\" )\n",
    "plt.grid(True, color=\"grey\", which='minor', linestyle=':', linewidth=\"0.5\")\n",
    "plt.minorticks_on()\n",
    "\n",
    "g.savefig('./Results/Figures/Precision_and_Recall_barplot.pdf', format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AUC & MCC bar chart**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_Scores = pd.read_csv(\"./Results/all/AUC_Scores.csv\")\n",
    "MCC_Scores  = pd.read_csv(\"./Results/all/MCC_Scores.csv\")\n",
    "\n",
    "models = ['DT', 'RF', 'ET', 'XGB', 'Ada', 'GB', 'HGB', 'CAT']\n",
    "\n",
    "for df in [AUC_Scores, MCC_Scores]:\n",
    "    for c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "mean_AUC_Scores = [float(AUC_Scores[m].mean()) for m in models]\n",
    "mean_MCC_Scores    = [float(MCC_Scores[m].mean()) for m in models]\n",
    "AUC_And_MCC_scores = mean_AUC_Scores + mean_MCC_Scores\n",
    "AUC_And_MCC_scores_DF = pd.DataFrame({'Classifier': models * 2, 'Score': AUC_And_MCC_scores,\n",
    "                                               'Evaluation Metrics': (['AUC'] * len(models)) + (['MCC'] * len(models))})\n",
    "\n",
    "sns.set(style='ticks')\n",
    "g = sns.catplot(x='Classifier', y='Score', hue='Evaluation Metrics', data=AUC_And_MCC_scores_DF, kind='bar', height=4, \n",
    "                aspect=2.5, palette=\"PuBu\")\n",
    "\n",
    "ax = g.facet_axis(0, 0)\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    if (h is None) or np.isnan(h) or h <= 0:\n",
    "        continue\n",
    "    ax.text(p.get_x() + p.get_width() / 2, h / 2, f\"{h:.4f}\", color='black', ha=\"center\", va=\"center\", rotation='vertical', size='small')\n",
    "\n",
    "plt.title('AUC and MCC for Each Tree-based Classifier', fontsize=12)\n",
    "plt.grid(True, color=\"grey\", which='major', linewidth=\"0.3\", linestyle=\"-.\" )\n",
    "plt.grid(True, color=\"grey\", which='minor', linestyle=':', linewidth=\"0.5\")\n",
    "plt.minorticks_on()\n",
    "\n",
    "g.savefig('./Results/Figures/AUC_and_MCC_barplot.pdf', format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Boxplots for Precision, Recall, AUC, MCC**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./Results/all/\"\n",
    "\n",
    "Precision_Scores = pd.read_csv(base_path + \"Precision_Scores.csv\")\n",
    "Recall_Scores    = pd.read_csv(base_path + \"Recall_Scores.csv\")\n",
    "AUC_Scores       = pd.read_csv(base_path + \"AUC_Scores.csv\")\n",
    "MCC_Scores       = pd.read_csv(base_path + \"MCC_Scores.csv\")\n",
    "\n",
    "models = ['DT', 'RF', 'ET', 'XGB', 'Ada', 'GB', 'HGB', 'CAT']\n",
    "\n",
    "for df in [Precision_Scores, Recall_Scores, AUC_Scores, MCC_Scores]:\n",
    "    for c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "n_rows = 2\n",
    "n_cols = 2\n",
    "gs = gridspec.GridSpec(n_rows, n_cols)\n",
    "scale = max(n_cols, n_rows)\n",
    "fig = plt.figure(figsize=(2 * scale, 2 * scale))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'AUC', 'MCC']\n",
    "metricsData = [Precision_Scores, Recall_Scores, AUC_Scores, MCC_Scores]\n",
    "n_plots = len(metrics)\n",
    "\n",
    "scale = max(n_cols, n_rows)\n",
    "fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(7 * scale, 5 * scale))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < n_plots:\n",
    "        g = sns.boxplot(data=metricsData[i], orient=\"h\", ax=ax, palette=\"Pastel2\", order=models)\n",
    "        # g.set(title='')\n",
    "        g.set(ylabel='Classifier')\n",
    "        g.set(xlabel=metrics[i])\n",
    "\n",
    "        ax.minorticks_on()\n",
    "        ax.grid(True, color=\"grey\", which='major', linewidth=\"0.3\", linestyle=\":\")\n",
    "        ax.grid(True, color=\"grey\", which='minor', linestyle=\":\", linewidth=\"0.5\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('./Results/Figures/Precision_Recall_AUC_MCC_Boxplots.pdf', format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Detection time bar chart**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_Scores = pd.read_csv(\"./Results/all/INF_TIME_Scores.csv\")\n",
    "models = ['DT', 'RF', 'ET', 'XGB', 'Ada', 'GB', 'HGB', 'CAT']\n",
    "for c in Time_Scores.columns:\n",
    "    Time_Scores[c] = pd.to_numeric(Time_Scores[c], errors=\"coerce\")\n",
    "\n",
    "Models_Time_Score = pd.DataFrame({\"Classifier\": models, \"Scoring Time (µs)\": [Time_Scores[m].mean() * 1e6 for m in models]})\n",
    "\n",
    "sns.set(style='ticks')\n",
    "g = sns.catplot(x='Classifier', y='Scoring Time (µs)', data=Models_Time_Score, kind='bar', height=5, aspect=1.2,palette=\"PuBu\")\n",
    "\n",
    "ax = g.facet_axis(0, 0)\n",
    "\n",
    "# Disable scientific notation completely\n",
    "ax.ticklabel_format(style='plain', axis='y', useOffset=False)\n",
    "\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    if h <= 0:\n",
    "        continue\n",
    "    ax.text(p.get_x() + p.get_width() / 2, h / 2, f\"{h:.2f}\", ha=\"center\", va=\"center\", rotation=\"vertical\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.title('Scoring Time for Each Tree-based Classifier', fontsize=12)\n",
    "plt.ylabel('Scoring Time (µs)')\n",
    "plt.grid(True, color=\"grey\", which='major', linewidth=0.3, linestyle='-.')\n",
    "plt.grid(True, color=\"grey\", which='minor', linestyle=':', linewidth=0.5)\n",
    "plt.minorticks_on()\n",
    "\n",
    "g.savefig('./Results/Figures/Scoring_Time_Tree_Based_Ensembles.pdf', format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_Scores = pd.read_csv(\"./Results/all/INF_TIME_Scores.csv\")\n",
    "\n",
    "models = ['DT', 'RF', 'ET', 'XGB', 'Ada', 'GB', 'HGB', 'CAT']\n",
    "\n",
    "for c in Time_Scores.columns: Time_Scores[c] = pd.to_numeric(Time_Scores[c], errors=\"coerce\")\n",
    "\n",
    "Models_Time_Score = pd.DataFrame({\"Classifier\": models, \"Scoring Time\": [Time_Scores[m].mean() for m in models]})\n",
    "\n",
    "sns.set(style='ticks')\n",
    "g = sns.catplot( x='Classifier', y='Scoring Time', data=Models_Time_Score, kind='bar', height=5, aspect=1.2, palette=\"PuBu\")\n",
    "ax = g.facet_axis(0, 0)\n",
    "ax.ticklabel_format(style='plain', axis='y', useOffset=False)\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width() / 2, p.get_height() * 0.5, f\"{p.get_height():.6f}\", ha=\"center\", va=\"center\", rotation=\"vertical\",\n",
    "            color=\"black\", size=\"medium\")\n",
    "\n",
    "plt.title('Scoring Time for Each Tree-based Classifier', fontsize=12)\n",
    "plt.grid(True, color=\"grey\", which='major', linewidth=\"0.3\", linestyle=\"-.\" )\n",
    "plt.grid(True, color=\"grey\", which='minor', linestyle=':', linewidth=\"0.5\")\n",
    "plt.minorticks_on()\n",
    "\n",
    "g.savefig('./Results/Figures/Scoring_Time_Tree_Based_Ensembles.pdf', format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
